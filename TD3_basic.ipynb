{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgKduPC9FdLc"
      },
      "source": [
        "TD3 Model\n",
        "@inproceedings{fujimoto2018addressing,\n",
        "  title={Addressing Function Approximation Error in Actor-Critic Methods},\n",
        "  author={Fujimoto, Scott and Hoof, Herke and Meger, David},\n",
        "  booktitle={International Conference on Machine Learning},\n",
        "  pages={1582--1591},\n",
        "  year={2018}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71yOuP6hFCPn"
      },
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Implementation of Twin Delayed Deep Deterministic Policy Gradients (TD3)\n",
        "# Paper: https://arxiv.org/abs/1802.09477\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\tdef __init__(self, state_dim, action_dim, max_action):\n",
        "\t\tsuper(Actor, self).__init__()\n",
        "\n",
        "\t\tself.l1 = nn.Linear(state_dim, 256)\n",
        "\t\tself.l2 = nn.Linear(256, 256)\n",
        "\t\tself.l3 = nn.Linear(256, action_dim)\n",
        "\t\t\n",
        "\t\tself.max_action = max_action\n",
        "\t\t\n",
        "\n",
        "\tdef forward(self, state):\n",
        "\t\ta = F.relu(self.l1(state))\n",
        "\t\ta = F.relu(self.l2(a))\n",
        "\t\treturn self.max_action * torch.tanh(self.l3(a))\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\tdef __init__(self, state_dim, action_dim):\n",
        "\t\tsuper(Critic, self).__init__()\n",
        "\n",
        "\t\t# Q1 architecture\n",
        "\t\tself.l1 = nn.Linear(state_dim + action_dim, 256)\n",
        "\t\tself.l2 = nn.Linear(256, 256)\n",
        "\t\tself.l3 = nn.Linear(256, 1)\n",
        "\n",
        "\t\t# Q2 architecture\n",
        "\t\tself.l4 = nn.Linear(state_dim + action_dim, 256)\n",
        "\t\tself.l5 = nn.Linear(256, 256)\n",
        "\t\tself.l6 = nn.Linear(256, 1)\n",
        "\n",
        "\n",
        "\tdef forward(self, state, action):\n",
        "\t\tsa = torch.cat([state, action], 1)\n",
        "\n",
        "\t\tq1 = F.relu(self.l1(sa))\n",
        "\t\tq1 = F.relu(self.l2(q1))\n",
        "\t\tq1 = self.l3(q1)\n",
        "\n",
        "\t\tq2 = F.relu(self.l4(sa))\n",
        "\t\tq2 = F.relu(self.l5(q2))\n",
        "\t\tq2 = self.l6(q2)\n",
        "\t\treturn q1, q2\n",
        "\n",
        "\n",
        "\tdef Q1(self, state, action):\n",
        "\t\tsa = torch.cat([state, action], 1)\n",
        "\n",
        "\t\tq1 = F.relu(self.l1(sa))\n",
        "\t\tq1 = F.relu(self.l2(q1))\n",
        "\t\tq1 = self.l3(q1)\n",
        "\t\treturn q1\n",
        "\n",
        "\n",
        "class TD3(object):\n",
        "\tdef __init__(\n",
        "\t\tself,\n",
        "\t\tstate_dim,\n",
        "\t\taction_dim,\n",
        "\t\tmax_action,\n",
        "\t\tdiscount=0.99,\n",
        "\t\ttau=0.005,\n",
        "\t\tpolicy_noise=0.2,\n",
        "\t\tnoise_clip=0.5,\n",
        "\t\tpolicy_freq=2\n",
        "\t):\n",
        "\n",
        "\t\tself.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
        "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "\n",
        "\t\tself.critic = Critic(state_dim, action_dim).to(device)\n",
        "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
        "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "\n",
        "\t\tself.max_action = max_action\n",
        "\t\tself.discount = discount\n",
        "\t\tself.tau = tau\n",
        "\t\tself.policy_noise = policy_noise\n",
        "\t\tself.noise_clip = noise_clip\n",
        "\t\tself.policy_freq = policy_freq\n",
        "\n",
        "\t\tself.total_it = 0\n",
        "\n",
        "\n",
        "\tdef select_action(self, state):\n",
        "\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "\n",
        "\tdef train(self, replay_buffer, batch_size=100):\n",
        "\t\tself.total_it += 1\n",
        "\n",
        "\t\t# Sample replay buffer \n",
        "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\t# Select action according to policy and add clipped noise\n",
        "\t\t\tnoise = (\n",
        "\t\t\t\ttorch.randn_like(action) * self.policy_noise\n",
        "\t\t\t).clamp(-self.noise_clip, self.noise_clip)\n",
        "\t\t\t\n",
        "\t\t\tnext_action = (\n",
        "\t\t\t\tself.actor_target(next_state) + noise\n",
        "\t\t\t).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "\t\t\t# Compute the target Q value\n",
        "\t\t\ttarget_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\t\t\ttarget_Q = torch.min(target_Q1, target_Q2)\n",
        "\t\t\ttarget_Q = reward + not_done * self.discount * target_Q\n",
        "\n",
        "\t\t# Get current Q estimates\n",
        "\t\tcurrent_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "\t\t# Compute critic loss\n",
        "\t\tcritic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "\t\t# Optimize the critic\n",
        "\t\tself.critic_optimizer.zero_grad()\n",
        "\t\tcritic_loss.backward()\n",
        "\t\tself.critic_optimizer.step()\n",
        "\n",
        "\t\t# Delayed policy updates\n",
        "\t\tif self.total_it % self.policy_freq == 0:\n",
        "\n",
        "\t\t\t# Compute actor losse\n",
        "\t\t\tactor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "\t\t\t\n",
        "\t\t\t# Optimize the actor \n",
        "\t\t\tself.actor_optimizer.zero_grad()\n",
        "\t\t\tactor_loss.backward()\n",
        "\t\t\tself.actor_optimizer.step()\n",
        "\n",
        "\t\t\t# Update the frozen target models\n",
        "\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "\n",
        "\tdef save(self, filename):\n",
        "\t\ttorch.save(self.critic.state_dict(), filename + \"_critic\")\n",
        "\t\ttorch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
        "\t\t\n",
        "\t\ttorch.save(self.actor.state_dict(), filename + \"_actor\")\n",
        "\t\ttorch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
        "\n",
        "\n",
        "\tdef load(self, filename):\n",
        "\t\tself.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
        "\t\tself.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
        "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
        "\n",
        "\t\tself.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
        "\t\tself.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
        "\t\tself.actor_target = copy.deepcopy(self.actor)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPkUNOugJtZc"
      },
      "source": [
        "Replay Buffer (from the same repo as TD3 Model)\n",
        "@inproceedings{fujimoto2018addressing,\n",
        "  title={Addressing Function Approximation Error in Actor-Critic Methods},\n",
        "  author={Fujimoto, Scott and Hoof, Herke and Meger, David},\n",
        "  booktitle={International Conference on Machine Learning},\n",
        "  pages={1582--1591},\n",
        "  year={2018}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rrh63qUuJoRN"
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
        "\t\tself.max_size = max_size\n",
        "\t\tself.ptr = 0\n",
        "\t\tself.size = 0\n",
        "\n",
        "\t\tself.state = np.zeros((max_size, state_dim))\n",
        "\t\tself.action = np.zeros((max_size, action_dim))\n",
        "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
        "\t\tself.reward = np.zeros((max_size, 1))\n",
        "\t\tself.not_done = np.zeros((max_size, 1))\n",
        "\n",
        "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\tdef add(self, state, action, next_state, reward, done):\n",
        "\t\tself.state[self.ptr] = state\n",
        "\t\tself.action[self.ptr] = action\n",
        "\t\tself.next_state[self.ptr] = next_state\n",
        "\t\tself.reward[self.ptr] = reward\n",
        "\t\tself.not_done[self.ptr] = 1. - done\n",
        "\n",
        "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
        "\t\tself.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "\n",
        "\tdef sample(self, batch_size):\n",
        "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
        "\n",
        "\t\treturn (\n",
        "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n",
        "\t\t)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOP_JBd8OQg_"
      },
      "source": [
        "#Code for policy evaluation\n",
        "# Runs policy for X episodes and returns average reward\n",
        "# A fixed seed is used for the eval environment\n",
        "def eval_policy(policy, env_name, seed, eval_episodes=10):\n",
        "\teval_env = gym.make(env_name)\n",
        "\teval_env.seed(seed + 100) #get a different seed from before\n",
        "\n",
        "\tavg_reward = 0.\n",
        "\tfor _ in range(eval_episodes):\n",
        "\t\tstate, done = eval_env.reset(), False\n",
        "\t\twhile not done:\n",
        "\t\t\taction = policy.select_action(np.array(state))\n",
        "\t\t\tstate, reward, done, _ = eval_env.step(action)\n",
        "\t\t\tavg_reward += reward\n",
        "\n",
        "\tavg_reward /= eval_episodes\n",
        "\n",
        "\tprint(\"---------------------------------------\")\n",
        "\tprint(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
        "\tprint(\"---------------------------------------\")\n",
        "\treturn avg_reward"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioGrqJ0tHzlE",
        "outputId": "9275afcc-14c4-4d38-f8bc-7cce4c38d179"
      },
      "source": [
        "!pip3 install numpngw\n",
        "from numpngw import write_apng\n",
        "import gym"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpngw in /usr/local/lib/python3.6/dist-packages (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from numpngw) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ1weNbQH2EZ",
        "outputId": "789ba49b-5689-47a0-9723-bd10c8dc2a4e"
      },
      "source": [
        "#Create the environment\n",
        "env_id = \"Pendulum-v0\"\n",
        "env = gym.make(env_id)\n",
        "state_dim = env.observation_space.shape[-1]\n",
        "action_dim = env.action_space.shape[-1]\n",
        "max_action=env.max_torque\n",
        "print('State dimension', state_dim)\n",
        "print('Action dimension', action_dim)\n",
        "print('Max action', max_action)\n",
        "print('Max number of episodes', env._max_episode_steps)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State dimension 3\n",
            "Action dimension 1\n",
            "Max action 2.0\n",
            "Max number of episodes 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te-6EhbrMqk9"
      },
      "source": [
        "#Seed environment, torch, and numpy for consistent results \n",
        "#Will need to find an optimal seed eventually\n",
        "seed=0 # Sets Gym, PyTorch and Numpy seeds\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJHn4pEmNnkp"
      },
      "source": [
        "#Define all the hyperparameters (currently default from the repo this code was taken)\n",
        "start_timesteps = 25e3 # Time steps initial random policy is used\n",
        "expl_noise = 0.1  # Std of Gaussian exploration noise\n",
        "policy_noise = 0.2 # Noise added to target policy during critic update\n",
        "batch_size = 256 # Batch size for both actor and critic\n",
        "noise_clip = 0.5 # Range to clip target policy noise\n",
        "policy_freq = 2 # Frequency of delayed policy updates\n",
        "tau = 0.005 # Target network update rate\n",
        "discount = 0.99 # Discount factor\n",
        "eval_freq = 5e3 # How often (time steps) we evaluate\n",
        "\n",
        "max_timesteps = 1e6 # Max time steps to run environment\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0lrXJhiLoZo"
      },
      "source": [
        "\tkwargs = {\n",
        "\t\t\"state_dim\": state_dim,\n",
        "\t\t\"action_dim\": action_dim,\n",
        "\t\t\"max_action\": max_action,\n",
        "\t\t\"discount\": discount,\n",
        "\t\t\"tau\": tau,\n",
        "\t}\n",
        "\n",
        "#Init model with the appropriate env variables and previously defined hyperparameters\n",
        "\n",
        "# Target policy smoothing is scaled wrt the action scale\n",
        "kwargs[\"policy_noise\"] = policy_noise * max_action\n",
        "kwargs[\"noise_clip\"] = noise_clip * max_action\n",
        "kwargs[\"policy_freq\"] = policy_freq\n",
        "policy = TD3(**kwargs)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-g563L-OqhJ"
      },
      "source": [
        "#To save models while training:\n",
        "save_model = True\n",
        "file_name = 'Test'"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRn3ab5qFz_O",
        "outputId": "4505290c-2605-4a3c-ae9a-791e2ddda1db"
      },
      "source": [
        "#Training\n",
        "replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
        "\t\n",
        "# Evaluate untrained policy\n",
        "evaluations = []\n",
        "\n",
        "state, done = env.reset(), False\n",
        "episode_reward = 0\n",
        "episode_timesteps = 0\n",
        "episode_num = 0\n",
        "\n",
        "for t in range(int(max_timesteps)):\n",
        "  \n",
        "  episode_timesteps += 1\n",
        "\n",
        "  # Select action randomly or according to policy\n",
        "  if t < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else:\n",
        "    action = (\n",
        "      policy.select_action(np.array(state))\n",
        "      + np.random.normal(0, max_action * expl_noise, size=action_dim)\n",
        "    ).clip(-max_action, max_action)\n",
        "\n",
        "  # Perform action\n",
        "  next_state, reward, done, _ = env.step(action) \n",
        "  done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
        "\n",
        "  # Store data in replay buffer\n",
        "  replay_buffer.add(state, action, next_state, reward, done_bool)\n",
        "\n",
        "  state = next_state\n",
        "  episode_reward += reward\n",
        "\n",
        "  # Train agent after collecting sufficient data\n",
        "  if t >= start_timesteps:\n",
        "    policy.train(replay_buffer, batch_size)\n",
        "\n",
        "  if done: \n",
        "    # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
        "    print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
        "    # Reset environment\n",
        "    state, done = env.reset(), False\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1 \n",
        "\n",
        "  # Evaluate episode\n",
        "  if (t + 1) % eval_freq == 0:\n",
        "    evaluations.append(eval_policy(policy, env_id, seed))\n",
        "    np.save(f\"results_{t+1}_{file_name}\", evaluations)\n",
        "    if save_model:\n",
        "       policy.save(f\"models_{t+1}_{file_name}\")\n",
        "      #  files.download(f'{name}.zip') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total T: 200 Episode Num: 1 Episode T: 200 Reward: -1578.401\n",
            "Total T: 400 Episode Num: 2 Episode T: 200 Reward: -971.775\n",
            "Total T: 600 Episode Num: 3 Episode T: 200 Reward: -1599.040\n",
            "Total T: 800 Episode Num: 4 Episode T: 200 Reward: -1009.411\n",
            "Total T: 1000 Episode Num: 5 Episode T: 200 Reward: -1711.823\n",
            "Total T: 1200 Episode Num: 6 Episode T: 200 Reward: -1493.793\n",
            "Total T: 1400 Episode Num: 7 Episode T: 200 Reward: -862.868\n",
            "Total T: 1600 Episode Num: 8 Episode T: 200 Reward: -1710.885\n",
            "Total T: 1800 Episode Num: 9 Episode T: 200 Reward: -911.380\n",
            "Total T: 2000 Episode Num: 10 Episode T: 200 Reward: -1552.590\n",
            "Total T: 2200 Episode Num: 11 Episode T: 200 Reward: -1280.946\n",
            "Total T: 2400 Episode Num: 12 Episode T: 200 Reward: -918.346\n",
            "Total T: 2600 Episode Num: 13 Episode T: 200 Reward: -1606.409\n",
            "Total T: 2800 Episode Num: 14 Episode T: 200 Reward: -1249.019\n",
            "Total T: 3000 Episode Num: 15 Episode T: 200 Reward: -1046.533\n",
            "Total T: 3200 Episode Num: 16 Episode T: 200 Reward: -1159.737\n",
            "Total T: 3400 Episode Num: 17 Episode T: 200 Reward: -1167.170\n",
            "Total T: 3600 Episode Num: 18 Episode T: 200 Reward: -1006.641\n",
            "Total T: 3800 Episode Num: 19 Episode T: 200 Reward: -1628.645\n",
            "Total T: 4000 Episode Num: 20 Episode T: 200 Reward: -975.937\n",
            "Total T: 4200 Episode Num: 21 Episode T: 200 Reward: -989.228\n",
            "Total T: 4400 Episode Num: 22 Episode T: 200 Reward: -1771.353\n",
            "Total T: 4600 Episode Num: 23 Episode T: 200 Reward: -1651.926\n",
            "Total T: 4800 Episode Num: 24 Episode T: 200 Reward: -1656.555\n",
            "Total T: 5000 Episode Num: 25 Episode T: 200 Reward: -988.701\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -1690.832\n",
            "---------------------------------------\n",
            "Total T: 5200 Episode Num: 26 Episode T: 200 Reward: -1656.405\n",
            "Total T: 5400 Episode Num: 27 Episode T: 200 Reward: -1499.537\n",
            "Total T: 5600 Episode Num: 28 Episode T: 200 Reward: -1165.885\n",
            "Total T: 5800 Episode Num: 29 Episode T: 200 Reward: -1342.281\n",
            "Total T: 6000 Episode Num: 30 Episode T: 200 Reward: -1718.278\n",
            "Total T: 6200 Episode Num: 31 Episode T: 200 Reward: -1412.411\n",
            "Total T: 6400 Episode Num: 32 Episode T: 200 Reward: -1512.091\n",
            "Total T: 6600 Episode Num: 33 Episode T: 200 Reward: -934.353\n",
            "Total T: 6800 Episode Num: 34 Episode T: 200 Reward: -1437.062\n",
            "Total T: 7000 Episode Num: 35 Episode T: 200 Reward: -1673.537\n",
            "Total T: 7200 Episode Num: 36 Episode T: 200 Reward: -1361.035\n",
            "Total T: 7400 Episode Num: 37 Episode T: 200 Reward: -1716.880\n",
            "Total T: 7600 Episode Num: 38 Episode T: 200 Reward: -1001.960\n",
            "Total T: 7800 Episode Num: 39 Episode T: 200 Reward: -1220.620\n",
            "Total T: 8000 Episode Num: 40 Episode T: 200 Reward: -1263.733\n",
            "Total T: 8200 Episode Num: 41 Episode T: 200 Reward: -1735.324\n",
            "Total T: 8400 Episode Num: 42 Episode T: 200 Reward: -1651.474\n",
            "Total T: 8600 Episode Num: 43 Episode T: 200 Reward: -1317.784\n",
            "Total T: 8800 Episode Num: 44 Episode T: 200 Reward: -876.436\n",
            "Total T: 9000 Episode Num: 45 Episode T: 200 Reward: -861.769\n",
            "Total T: 9200 Episode Num: 46 Episode T: 200 Reward: -962.719\n",
            "Total T: 9400 Episode Num: 47 Episode T: 200 Reward: -1341.393\n",
            "Total T: 9600 Episode Num: 48 Episode T: 200 Reward: -856.681\n",
            "Total T: 9800 Episode Num: 49 Episode T: 200 Reward: -869.545\n",
            "Total T: 10000 Episode Num: 50 Episode T: 200 Reward: -913.675\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -1690.832\n",
            "---------------------------------------\n",
            "Total T: 10200 Episode Num: 51 Episode T: 200 Reward: -1841.365\n",
            "Total T: 10400 Episode Num: 52 Episode T: 200 Reward: -1068.685\n",
            "Total T: 10600 Episode Num: 53 Episode T: 200 Reward: -1357.531\n",
            "Total T: 10800 Episode Num: 54 Episode T: 200 Reward: -1160.816\n",
            "Total T: 11000 Episode Num: 55 Episode T: 200 Reward: -1227.380\n",
            "Total T: 11200 Episode Num: 56 Episode T: 200 Reward: -1794.093\n",
            "Total T: 11400 Episode Num: 57 Episode T: 200 Reward: -894.158\n",
            "Total T: 11600 Episode Num: 58 Episode T: 200 Reward: -1459.540\n",
            "Total T: 11800 Episode Num: 59 Episode T: 200 Reward: -1187.929\n",
            "Total T: 12000 Episode Num: 60 Episode T: 200 Reward: -1198.240\n",
            "Total T: 12200 Episode Num: 61 Episode T: 200 Reward: -853.455\n",
            "Total T: 12400 Episode Num: 62 Episode T: 200 Reward: -1267.127\n",
            "Total T: 12600 Episode Num: 63 Episode T: 200 Reward: -1395.603\n",
            "Total T: 12800 Episode Num: 64 Episode T: 200 Reward: -1199.507\n",
            "Total T: 13000 Episode Num: 65 Episode T: 200 Reward: -1316.045\n",
            "Total T: 13200 Episode Num: 66 Episode T: 200 Reward: -1070.393\n",
            "Total T: 13400 Episode Num: 67 Episode T: 200 Reward: -865.552\n",
            "Total T: 13600 Episode Num: 68 Episode T: 200 Reward: -1360.515\n",
            "Total T: 13800 Episode Num: 69 Episode T: 200 Reward: -1647.479\n",
            "Total T: 14000 Episode Num: 70 Episode T: 200 Reward: -1274.587\n",
            "Total T: 14200 Episode Num: 71 Episode T: 200 Reward: -1166.018\n",
            "Total T: 14400 Episode Num: 72 Episode T: 200 Reward: -1063.254\n",
            "Total T: 14600 Episode Num: 73 Episode T: 200 Reward: -1555.022\n",
            "Total T: 14800 Episode Num: 74 Episode T: 200 Reward: -1163.905\n",
            "Total T: 15000 Episode Num: 75 Episode T: 200 Reward: -1254.698\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -1690.832\n",
            "---------------------------------------\n",
            "Total T: 15200 Episode Num: 76 Episode T: 200 Reward: -990.244\n",
            "Total T: 15400 Episode Num: 77 Episode T: 200 Reward: -1337.389\n",
            "Total T: 15600 Episode Num: 78 Episode T: 200 Reward: -745.067\n",
            "Total T: 15800 Episode Num: 79 Episode T: 200 Reward: -1313.365\n",
            "Total T: 16000 Episode Num: 80 Episode T: 200 Reward: -1070.259\n",
            "Total T: 16200 Episode Num: 81 Episode T: 200 Reward: -1714.717\n",
            "Total T: 16400 Episode Num: 82 Episode T: 200 Reward: -923.917\n",
            "Total T: 16600 Episode Num: 83 Episode T: 200 Reward: -1823.143\n",
            "Total T: 16800 Episode Num: 84 Episode T: 200 Reward: -1083.461\n",
            "Total T: 17000 Episode Num: 85 Episode T: 200 Reward: -1223.815\n",
            "Total T: 17200 Episode Num: 86 Episode T: 200 Reward: -1059.303\n",
            "Total T: 17400 Episode Num: 87 Episode T: 200 Reward: -1644.469\n",
            "Total T: 17600 Episode Num: 88 Episode T: 200 Reward: -1072.402\n",
            "Total T: 17800 Episode Num: 89 Episode T: 200 Reward: -873.427\n",
            "Total T: 18000 Episode Num: 90 Episode T: 200 Reward: -848.502\n",
            "Total T: 18200 Episode Num: 91 Episode T: 200 Reward: -1016.456\n",
            "Total T: 18400 Episode Num: 92 Episode T: 200 Reward: -817.838\n",
            "Total T: 18600 Episode Num: 93 Episode T: 200 Reward: -1400.679\n",
            "Total T: 18800 Episode Num: 94 Episode T: 200 Reward: -880.294\n",
            "Total T: 19000 Episode Num: 95 Episode T: 200 Reward: -911.644\n",
            "Total T: 19200 Episode Num: 96 Episode T: 200 Reward: -1523.381\n",
            "Total T: 19400 Episode Num: 97 Episode T: 200 Reward: -872.521\n",
            "Total T: 19600 Episode Num: 98 Episode T: 200 Reward: -1311.269\n",
            "Total T: 19800 Episode Num: 99 Episode T: 200 Reward: -868.062\n",
            "Total T: 20000 Episode Num: 100 Episode T: 200 Reward: -1554.396\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -1690.832\n",
            "---------------------------------------\n",
            "Total T: 20200 Episode Num: 101 Episode T: 200 Reward: -1291.871\n",
            "Total T: 20400 Episode Num: 102 Episode T: 200 Reward: -1252.620\n",
            "Total T: 20600 Episode Num: 103 Episode T: 200 Reward: -1572.792\n",
            "Total T: 20800 Episode Num: 104 Episode T: 200 Reward: -1808.134\n",
            "Total T: 21000 Episode Num: 105 Episode T: 200 Reward: -1492.492\n",
            "Total T: 21200 Episode Num: 106 Episode T: 200 Reward: -983.126\n",
            "Total T: 21400 Episode Num: 107 Episode T: 200 Reward: -1254.873\n",
            "Total T: 21600 Episode Num: 108 Episode T: 200 Reward: -1488.559\n",
            "Total T: 21800 Episode Num: 109 Episode T: 200 Reward: -924.019\n",
            "Total T: 22000 Episode Num: 110 Episode T: 200 Reward: -1627.262\n",
            "Total T: 22200 Episode Num: 111 Episode T: 200 Reward: -961.980\n",
            "Total T: 22400 Episode Num: 112 Episode T: 200 Reward: -903.968\n",
            "Total T: 22600 Episode Num: 113 Episode T: 200 Reward: -1286.090\n",
            "Total T: 22800 Episode Num: 114 Episode T: 200 Reward: -1460.999\n",
            "Total T: 23000 Episode Num: 115 Episode T: 200 Reward: -966.873\n",
            "Total T: 23200 Episode Num: 116 Episode T: 200 Reward: -1692.645\n",
            "Total T: 23400 Episode Num: 117 Episode T: 200 Reward: -1494.320\n",
            "Total T: 23600 Episode Num: 118 Episode T: 200 Reward: -970.645\n",
            "Total T: 23800 Episode Num: 119 Episode T: 200 Reward: -1368.547\n",
            "Total T: 24000 Episode Num: 120 Episode T: 200 Reward: -862.209\n",
            "Total T: 24200 Episode Num: 121 Episode T: 200 Reward: -1411.133\n",
            "Total T: 24400 Episode Num: 122 Episode T: 200 Reward: -1179.402\n",
            "Total T: 24600 Episode Num: 123 Episode T: 200 Reward: -1289.080\n",
            "Total T: 24800 Episode Num: 124 Episode T: 200 Reward: -1456.565\n",
            "Total T: 25000 Episode Num: 125 Episode T: 200 Reward: -864.656\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -1690.832\n",
            "---------------------------------------\n",
            "Total T: 25200 Episode Num: 126 Episode T: 200 Reward: -1495.139\n",
            "Total T: 25400 Episode Num: 127 Episode T: 200 Reward: -1598.240\n",
            "Total T: 25600 Episode Num: 128 Episode T: 200 Reward: -1715.233\n",
            "Total T: 25800 Episode Num: 129 Episode T: 200 Reward: -1791.907\n",
            "Total T: 26000 Episode Num: 130 Episode T: 200 Reward: -1802.899\n",
            "Total T: 26200 Episode Num: 131 Episode T: 200 Reward: -1432.506\n",
            "Total T: 26400 Episode Num: 132 Episode T: 200 Reward: -1550.663\n",
            "Total T: 26600 Episode Num: 133 Episode T: 200 Reward: -1419.240\n",
            "Total T: 26800 Episode Num: 134 Episode T: 200 Reward: -1473.516\n",
            "Total T: 27000 Episode Num: 135 Episode T: 200 Reward: -1512.292\n",
            "Total T: 27200 Episode Num: 136 Episode T: 200 Reward: -1510.750\n",
            "Total T: 27400 Episode Num: 137 Episode T: 200 Reward: -1264.356\n",
            "Total T: 27600 Episode Num: 138 Episode T: 200 Reward: -1512.963\n",
            "Total T: 27800 Episode Num: 139 Episode T: 200 Reward: -1181.078\n",
            "Total T: 28000 Episode Num: 140 Episode T: 200 Reward: -1168.624\n",
            "Total T: 28200 Episode Num: 141 Episode T: 200 Reward: -1080.455\n",
            "Total T: 28400 Episode Num: 142 Episode T: 200 Reward: -1083.716\n",
            "Total T: 28600 Episode Num: 143 Episode T: 200 Reward: -1511.833\n",
            "Total T: 28800 Episode Num: 144 Episode T: 200 Reward: -893.754\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h4u5BLPGDZm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}