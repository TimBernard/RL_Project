{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom_pendulum.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWskDE2c9WoN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8e6a67d-ed6a-4c78-edd2-81e6e392a5a5"
      },
      "source": [
        "!pip install stable-baselines3[extra] pybullet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stable-baselines3[extra]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/7c/ec89fd9a51c2ff640f150479069be817136c02f02349b5dd27a6e3bb8b3d/stable_baselines3-0.10.0-py3-none-any.whl (145kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 5.3MB/s \n",
            "\u001b[?25hCollecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/d9/756b8fe29c574b34e3a60fd777688f8aaacb7eae37fcd1b5983ec415646d/pybullet-3.0.7-cp36-cp36m-manylinux1_x86_64.whl (87.5MB)\n",
            "\u001b[K     |████████████████████████████████| 87.5MB 59kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (1.7.0+cu101)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (1.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (1.18.5)\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (0.17.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"extra\" in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (0.2.6)\n",
            "Requirement already satisfied: tensorboard; extra == \"extra\" in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (2.3.0)\n",
            "Requirement already satisfied: psutil; extra == \"extra\" in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Requirement already satisfied: opencv-python; extra == \"extra\" in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (4.1.2.30)\n",
            "Requirement already satisfied: pillow; extra == \"extra\" in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (7.0.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3[extra]) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3[extra]) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->stable-baselines3[extra]) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->stable-baselines3[extra]) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->stable-baselines3[extra]) (0.8)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines3[extra]) (2018.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.17->stable-baselines3[extra]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.17->stable-baselines3[extra]) (1.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.10.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.17.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (50.3.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.3.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.33.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.35.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.12.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (2.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.1.0)\n",
            "Installing collected packages: stable-baselines3, pybullet\n",
            "Successfully installed pybullet-3.0.7 stable-baselines3-0.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMJjT8U6soTP",
        "outputId": "bd1c5af8-9c51-4571-8c5b-fd6872f233aa"
      },
      "source": [
        "import os \n",
        "\n",
        "from stable_baselines3.common.cmd_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import VecNormalize\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3 import DDPG\n",
        "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "!pip3 install numpngw\n",
        "from numpngw import write_apng\n",
        "\n",
        "import gym"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/stable_baselines3/common/cmd_util.py:6: FutureWarning: Module ``common.cmd_util`` has been renamed to ``common.env_util`` and will be removed in the future.\n",
            "  \"Module ``common.cmd_util`` has been renamed to ``common.env_util`` and will be removed in the future.\", FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting numpngw\n",
            "  Downloading https://files.pythonhosted.org/packages/48/99/a2482bbf4d3a663042f496e9a23fb68b068e8768baf0183293f3e5f9aaad/numpngw-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from numpngw) (1.18.5)\n",
            "Installing collected packages: numpngw\n",
            "Successfully installed numpngw-0.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7W1ZQzhpLuM"
      },
      "source": [
        "# Train on regular pendulum environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7XS_EDApqbx",
        "outputId": "0f252cfe-4369-44dc-a657-312ccd7395ac"
      },
      "source": [
        "env_id = \"Pendulum-v0\"\n",
        "env = gym.make(env_id)\n",
        "\n",
        "print(env.action_space.shape[-1])\n",
        "print(env.observation_space.shape[-1])\n",
        "print(env._max_episode_steps)\n",
        "print(env.max_torque)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "3\n",
            "200\n",
            "2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1bFoNtOvPtj",
        "outputId": "6ea3a39d-3506-4413-9d5d-d949972d1c79"
      },
      "source": [
        "# The noise objects for DDPG\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "# Create and Train Model \n",
        "piRL_model = DDPG(policy='MlpPolicy', env=env, action_noise=action_noise, verbose=1)\n",
        "time_steps = 1000\n",
        "piRL_model.learn(total_timesteps=time_steps) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 88       |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total timesteps | 800      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 18.5     |\n",
            "|    critic_loss     | 0.606    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 600      |\n",
            "---------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ddpg.ddpg.DDPG at 0x7f732f997828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvUPpAgup1xW"
      },
      "source": [
        "# Test on custom environment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WRcS_R2qIPk"
      },
      "source": [
        "## Install custom pendulum environment \n",
        "#import sys\n",
        "#project_path = '/content/drive/My Drive/dl/project'\n",
        "#sys.path.append(project_path)\n",
        "#!pip install -e gym-pendulum-ssrl"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWR5W1aZxpG7"
      },
      "source": [
        "# Or just paste it in here "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHsiMlrzz5yw"
      },
      "source": [
        "def angle_normalize(x):\n",
        "    return (((x+np.pi) % (2*np.pi)) - np.pi)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOr6RoJPxoaE"
      },
      "source": [
        "\"\"\" Modify Pendulum Environment to be able to use a mlp reward \"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import error, spaces, utils\n",
        "from gym.utils import seeding\n",
        "from gym.envs.classic_control.pendulum import PendulumEnv\n",
        "import gym.envs.classic_control.pendulum\n",
        "\n",
        "class PendulumSSRLEnv(PendulumEnv):\n",
        "    def __init__(self):\n",
        "        super(PendulumSSRLEnv, self).__init__()\n",
        "        self.T = env._max_episode_steps\n",
        "        self.RewardMLP = RewardNet((self.action_space.shape[-1] + self.observation_space.shape[-1])) \n",
        "        self.D_piRL = None\n",
        "        self.D_samp = None\n",
        "        self.step_ = 0\n",
        "        self.episode_train = 10\n",
        "        self.state_action_record = []\n",
        "\n",
        "    def step(self, u):\n",
        "        th, thdot = self.state  # th := theta\n",
        "\n",
        "        g = self.g\n",
        "        m = self.m\n",
        "        l = self.l\n",
        "        dt = self.dt\n",
        "\n",
        "        u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
        "        self.last_u = u  # for rendering\n",
        "        costs = angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2)\n",
        "\n",
        "        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt\n",
        "        newth = th + newthdot * dt\n",
        "        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)\n",
        "\n",
        "        self.state = np.array([newth, newthdot])\n",
        "\n",
        "        # Add code to:\n",
        "        #   * record state action results from the policy being learned \n",
        "        #   * add a row to D_samp after each episode \n",
        "        #   * stop and train every \"episode_train\" episodes \n",
        "        #   * record state \n",
        "        #   * Do a forward pass on the current state and action to get an estimated reward \n",
        "        self.state_action_record.extend((self._get_obs()[0], self._get_obs()[1], self._get_obs()[2], u)) \n",
        "        if ((self.step_ % self.T) == 0) and (self.step_ != 0):\n",
        "\n",
        "            #print(\"self.D_samp.shape \", self.D_samp.shape)\n",
        "            #print(\"np.array(self.state_action_record).reshape(1,-1) \", np.array(self.state_action_record).reshape(1,-1).shape)\n",
        "\n",
        "            np.concatenate((self.D_samp, np.array(self.state_action_record).reshape(1,-1)[:,:800]))\n",
        "            self.state_action_record = []\n",
        "        if ((self.step_ % (self.T * self.episode_train)) == 0) and (self.step_ != 0):\n",
        "            print(\"Training... \")\n",
        "            self.train_reward()\n",
        "\n",
        "        self.step_ += 1\n",
        "        example = torch.tensor([self._get_obs()[0], self._get_obs()[1], self._get_obs()[2], u]).float()\n",
        "        reward = float(self.RewardMLP(example))\n",
        "\n",
        "        return self._get_obs(), reward, False, {}\n",
        "        #return self._get_obs(), -costs, False, {}\n",
        "\n",
        "    def generate_piRL_samples(self, env_piRL, model_piRL, num_piRL_samples=30):\n",
        "        \n",
        "        self.D_piRL = np.empty([num_piRL_samples, self.T*(self.action_space.shape[-1] + self.observation_space.shape[-1])])\n",
        "        for i_episode in range(num_piRL_samples):\n",
        "            ep = []\n",
        "            obs = env_piRL.reset()\n",
        "            for t in range(self.T):\n",
        "                action, _states = model_piRL.predict(obs)\n",
        "                ep.append(np.concatenate((obs,action)))\n",
        "            \n",
        "            self.D_piRL[i_episode,:] = np.array(ep).reshape(1,-1)\n",
        "\n",
        "        self.D_samp = self.D_piRL.copy()\n",
        "\n",
        "    def train_reward(self, num_train_stps=10, batch_size=4):\n",
        "        \n",
        "        model = self.RewardMLP\n",
        "        loss_func = RewardLoss()\n",
        "        loss_func.requires_grad = True\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "        # Intialize Weights with He Initialization\n",
        "        def weights_init(m):\n",
        "            if (type(m) == torch.nn.Linear):\n",
        "                torch.nn.init.kaiming_uniform_(m.weight)\n",
        "        weights_init(model)\n",
        "\n",
        "        D_train = np.concatenate((self.D_piRL, self.D_samp))\n",
        "\n",
        "        for stp in range(num_train_stps):\n",
        "            i = np.random.choice(D_train.shape[0], size=batch_size, replace=False)\n",
        "            trajectory = torch.from_numpy(D_train[i].astype(np.float32)).float()\n",
        "            \n",
        "            # Compute Loss and train \n",
        "            loss = loss_func(self.D_piRL, self.D_samp, model)\n",
        "            print(\"Loss: \", loss)\n",
        "            optimizer.zero_grad()  \n",
        "            loss.backward()        \n",
        "            optimizer.step()       \n",
        "\n",
        "\n",
        "# Calculate sum of rewards for one episode \n",
        "def get_return(model, trajectory):\n",
        "    num_steps = trajectory.shape[-1]\n",
        "    return_sum = 0\n",
        "    for i in range(0, num_steps, 4):\n",
        "        return_sum += model(torch.from_numpy(trajectory[i:i+4]).float())\n",
        "    \n",
        "    return return_sum \n",
        "                \n",
        "\n",
        "class RewardLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RewardLoss, self).__init__()\n",
        "        self.max_torque = 2\n",
        "\n",
        "    def forward(self, D_piRL, D_samp, reward_model):\n",
        "        \"\"\" Calculate (negative) log-likelihood of trajectories, reward model parameters   \n",
        "        D_piRL: tensor of size (N, T*V)\n",
        "        D_samp: tensor of size (M, T*V)\n",
        "                N = num samples from piRL, M = num samples from piTheta (and piRL),\n",
        "                T = episode length, V = (size of action space) + (size of observation space)\n",
        "        reward_model: mlp, current estimate of reward function \n",
        "\n",
        "        return: Negative of eq. 3 from Generalizing Skills paper \n",
        "        \"\"\"\n",
        "\n",
        "        # Estimate partition function, Z \n",
        "        inner_sum = 0\n",
        "        for i in range(D_samp.shape[0]):\n",
        "\n",
        "            numerator = torch.exp(get_return(reward_model, D_samp[i,:]))\n",
        "            denominator = 1 / (self.max_torque - (-1.0 * self.max_torque))\n",
        "            inner_sum += (numerator/denominator)\n",
        "        \n",
        "        log_inner_sum = torch.log(inner_sum)\n",
        "\n",
        "        # Now estimate Likelihood\n",
        "        outer_sum = 0 \n",
        "        for i in range(D_piRL.shape[0]):\n",
        "            outer_sum += get_return(reward_model, D_piRL[i,:]) - log_inner_sum \n",
        "        \n",
        "        return -1.0 * outer_sum \n",
        "\n",
        "class RewardNet(nn.Module):\n",
        "    def __init__(self, input_size, h1_size=30, h2_size=30):\n",
        "        super(RewardNet, self).__init__()\n",
        "\n",
        "        # Inputs \n",
        "        self.input_size = input_size\n",
        "        self.h1_size = h1_size\n",
        "        self.h2_size = h2_size\n",
        "\n",
        "        # Fully Connected Layers \n",
        "        self.linear1 = nn.Linear(self.input_size, self.h1_size)\n",
        "        self.linear2 = nn.Linear(self.h1_size, self.h2_size)\n",
        "        self.linear3 = nn.Linear(self.h2_size, 1)\n",
        "\n",
        "        # Activations\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, input_data):\n",
        "        out = self.relu(self.linear1(input_data))\n",
        "        out = self.relu(self.linear2(out))\n",
        "        return self.linear3(out)\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DOpP_B0-LXm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "235d2bb4-9fe6-4f80-e827-9c15c6cea9c8"
      },
      "source": [
        "from stable_baselines3.common.env_checker import check_env\n",
        "custom_pendulum_env = PendulumSSRLEnv()\n",
        "check_env(custom_pendulum_env, warn=True)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/stable_baselines3/common/env_checker.py:232: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
            "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnFTnf1vzfgg"
      },
      "source": [
        "custom_pendulum_env.generate_piRL_samples(env_piRL=env, model_piRL=piRL_model)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDU4d_xm1wrJ",
        "outputId": "d436f052-82a8-41e3-f1ae-22def25f6f72"
      },
      "source": [
        "# The noise objects for DDPG\n",
        "n_actions = custom_pendulum_env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "# Create and Train Model \n",
        "piTheta_model = DDPG(policy='MlpPolicy', env=custom_pendulum_env, action_noise=action_noise, verbose=1)\n",
        "time_steps = 7\n",
        "piTheta_model.learn(total_timesteps=time_steps) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Training... \n",
            "Loss:  tensor([442.0237], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([645.9562], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([871.3030], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([565.5657], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([651.6309], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([641.4948], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([520.2474], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([495.3413], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([473.3184], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([395.4881], grad_fn=<MulBackward0>)\n",
            "Training... \n",
            "Loss:  tensor([326.4608], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([342.9280], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([415.3419], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([311.1623], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([221.3013], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([334.1561], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([301.0687], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([215.4916], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([218.5385], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([185.2621], grad_fn=<MulBackward0>)\n",
            "Training... \n",
            "Loss:  tensor([180.4917], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([558.2925], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([450.3281], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([294.4258], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([333.4687], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([319.2122], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([238.9344], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([200.0489], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([185.1258], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([187.1801], grad_fn=<MulBackward0>)\n",
            "Training... \n",
            "Loss:  tensor([178.4460], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([236.9793], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([257.3092], grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([220.0467], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}