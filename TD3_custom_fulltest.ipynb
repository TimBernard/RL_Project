{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3_custom_fulltest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgKduPC9FdLc"
      },
      "source": [
        "TD3 Model\n",
        "@inproceedings{fujimoto2018addressing,\n",
        "  title={Addressing Function Approximation Error in Actor-Critic Methods},\n",
        "  author={Fujimoto, Scott and Hoof, Herke and Meger, David},\n",
        "  booktitle={International Conference on Machine Learning},\n",
        "  pages={1582--1591},\n",
        "  year={2018}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLzcwECqwYOW"
      },
      "source": [
        "# TD3 Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71yOuP6hFCPn"
      },
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Implementation of Twin Delayed Deep Deterministic Policy Gradients (TD3)\n",
        "# Paper: https://arxiv.org/abs/1802.09477\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\tdef __init__(self, state_dim, action_dim, max_action):\n",
        "\t\tsuper(Actor, self).__init__()\n",
        "\n",
        "\t\tself.l1 = nn.Linear(state_dim, 256)\n",
        "\t\tself.l2 = nn.Linear(256, 256)\n",
        "\t\tself.l3 = nn.Linear(256, action_dim)\n",
        "\t\t\n",
        "\t\tself.max_action = max_action\n",
        "\t\t\n",
        "\n",
        "\tdef forward(self, state):\n",
        "\t\ta = F.relu(self.l1(state))\n",
        "\t\ta = F.relu(self.l2(a))\n",
        "\t\treturn self.max_action * torch.tanh(self.l3(a))\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\tdef __init__(self, state_dim, action_dim):\n",
        "\t\tsuper(Critic, self).__init__()\n",
        "\n",
        "\t\t# Q1 architecture\n",
        "\t\tself.l1 = nn.Linear(state_dim + action_dim, 256)\n",
        "\t\tself.l2 = nn.Linear(256, 256)\n",
        "\t\tself.l3 = nn.Linear(256, 1)\n",
        "\n",
        "\t\t# Q2 architecture\n",
        "\t\tself.l4 = nn.Linear(state_dim + action_dim, 256)\n",
        "\t\tself.l5 = nn.Linear(256, 256)\n",
        "\t\tself.l6 = nn.Linear(256, 1)\n",
        "\n",
        "\n",
        "\tdef forward(self, state, action):\n",
        "\t\tsa = torch.cat([state, action], 1)\n",
        "\n",
        "\t\tq1 = F.relu(self.l1(sa))\n",
        "\t\tq1 = F.relu(self.l2(q1))\n",
        "\t\tq1 = self.l3(q1)\n",
        "\n",
        "\t\tq2 = F.relu(self.l4(sa))\n",
        "\t\tq2 = F.relu(self.l5(q2))\n",
        "\t\tq2 = self.l6(q2)\n",
        "\t\treturn q1, q2\n",
        "\n",
        "\n",
        "\tdef Q1(self, state, action):\n",
        "\t\tsa = torch.cat([state, action], 1)\n",
        "\n",
        "\t\tq1 = F.relu(self.l1(sa))\n",
        "\t\tq1 = F.relu(self.l2(q1))\n",
        "\t\tq1 = self.l3(q1)\n",
        "\t\treturn q1\n",
        "\n",
        "\n",
        "class TD3(object):\n",
        "\tdef __init__(\n",
        "\t\tself,\n",
        "\t\tstate_dim,\n",
        "\t\taction_dim,\n",
        "\t\tmax_action,\n",
        "\t\tdiscount=0.99,\n",
        "\t\ttau=0.005,\n",
        "\t\tpolicy_noise=0.2,\n",
        "\t\tnoise_clip=0.5,\n",
        "\t\tpolicy_freq=2\n",
        "\t):\n",
        "\n",
        "\t\tself.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
        "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "\n",
        "\t\tself.critic = Critic(state_dim, action_dim).to(device)\n",
        "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
        "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "\n",
        "\t\tself.max_action = max_action\n",
        "\t\tself.discount = discount\n",
        "\t\tself.tau = tau\n",
        "\t\tself.policy_noise = policy_noise\n",
        "\t\tself.noise_clip = noise_clip\n",
        "\t\tself.policy_freq = policy_freq\n",
        "\n",
        "\t\tself.total_it = 0\n",
        "\n",
        "\n",
        "\tdef select_action(self, state):\n",
        "\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "\n",
        "\tdef train(self, replay_buffer, batch_size=100):\n",
        "\t\tself.total_it += 1\n",
        "\n",
        "\t\t# Sample replay buffer \n",
        "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\t# Select action according to policy and add clipped noise\n",
        "\t\t\tnoise = (\n",
        "\t\t\t\ttorch.randn_like(action) * self.policy_noise\n",
        "\t\t\t).clamp(-self.noise_clip, self.noise_clip)\n",
        "\t\t\t\n",
        "\t\t\tnext_action = (\n",
        "\t\t\t\tself.actor_target(next_state) + noise\n",
        "\t\t\t).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "\t\t\t# Compute the target Q value\n",
        "\t\t\ttarget_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\t\t\ttarget_Q = torch.min(target_Q1, target_Q2)\n",
        "\t\t\ttarget_Q = reward + not_done * self.discount * target_Q\n",
        "\n",
        "\t\t# Get current Q estimates\n",
        "\t\tcurrent_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "\t\t# Compute critic loss\n",
        "\t\tcritic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "\t\t# Optimize the critic\n",
        "\t\tself.critic_optimizer.zero_grad()\n",
        "\t\tcritic_loss.backward()\n",
        "\t\tself.critic_optimizer.step()\n",
        "\n",
        "\t\t# Delayed policy updates\n",
        "\t\tif self.total_it % self.policy_freq == 0:\n",
        "\n",
        "\t\t\t# Compute actor losse\n",
        "\t\t\tactor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "\t\t\t\n",
        "\t\t\t# Optimize the actor \n",
        "\t\t\tself.actor_optimizer.zero_grad()\n",
        "\t\t\tactor_loss.backward()\n",
        "\t\t\tself.actor_optimizer.step()\n",
        "\n",
        "\t\t\t# Update the frozen target models\n",
        "\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\t\telse: #to have output\n",
        "\t\t\tactor_loss = torch.tensor(np.nan).to(device)\n",
        "\n",
        "\t\treturn critic_loss.detach().cpu().numpy(), actor_loss.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "\tdef save(self, filename):\n",
        "\t\ttorch.save(self.critic.state_dict(), filename + \"_critic\")\n",
        "\t\ttorch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
        "\t\t\n",
        "\t\ttorch.save(self.actor.state_dict(), filename + \"_actor\")\n",
        "\t\ttorch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
        "\n",
        "\n",
        "\tdef load(self, filename):\n",
        "\t\tself.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
        "\t\tself.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
        "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
        "\n",
        "\t\tself.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
        "\t\tself.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
        "\t\tself.actor_target = copy.deepcopy(self.actor)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPkUNOugJtZc"
      },
      "source": [
        "Replay Buffer (from the same repo as TD3 Model)\n",
        "@inproceedings{fujimoto2018addressing,\n",
        "  title={Addressing Function Approximation Error in Actor-Critic Methods},\n",
        "  author={Fujimoto, Scott and Hoof, Herke and Meger, David},\n",
        "  booktitle={International Conference on Machine Learning},\n",
        "  pages={1582--1591},\n",
        "  year={2018}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rrh63qUuJoRN"
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
        "\t\tself.max_size = max_size\n",
        "\t\tself.ptr = 0\n",
        "\t\tself.size = 0\n",
        "\n",
        "\t\tself.state = np.zeros((max_size, state_dim))\n",
        "\t\tself.action = np.zeros((max_size, action_dim))\n",
        "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
        "\t\tself.reward = np.zeros((max_size, 1))\n",
        "\t\tself.not_done = np.zeros((max_size, 1))\n",
        "\n",
        "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\tdef add(self, state, action, next_state, reward, done):\n",
        "\t\tself.state[self.ptr] = state\n",
        "\t\tself.action[self.ptr] = action\n",
        "\t\tself.next_state[self.ptr] = next_state\n",
        "\t\tself.reward[self.ptr] = reward\n",
        "\t\tself.not_done[self.ptr] = 1. - done\n",
        "\n",
        "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
        "\t\tself.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "\n",
        "\tdef sample(self, batch_size):\n",
        "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
        "\n",
        "\t\treturn (\n",
        "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n",
        "\t\t)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOP_JBd8OQg_"
      },
      "source": [
        "#Code for policy evaluation\n",
        "# Runs policy for X episodes and returns average reward\n",
        "# A fixed seed is used for the eval environment\n",
        "def eval_policy(policy, env_name, seed, eval_episodes=10):\n",
        "\teval_env = gym.make(env_name)\n",
        "\teval_env.seed(seed + 100) #get a different seed from before\n",
        "\n",
        "\tavg_reward = 0.\n",
        "\tfor _ in range(eval_episodes):\n",
        "\t\tstate, done = eval_env.reset(), False\n",
        "\t\twhile not done:\n",
        "\t\t\taction = policy.select_action(np.array(state))\n",
        "\t\t\tstate, reward, done, _ = eval_env.step(action)\n",
        "\t\t\tavg_reward += reward\n",
        "\n",
        "\tavg_reward /= eval_episodes\n",
        "\n",
        "\tprint(\"---------------------------------------\")\n",
        "\tprint(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
        "\tprint(\"---------------------------------------\")\n",
        "\treturn avg_reward"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1sQO-nRr_d2"
      },
      "source": [
        "# The Fun Part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VHjDUGAvRVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e03b39f-47ba-4be9-e635-8a6c7b927b09"
      },
      "source": [
        "!pip install stable-baselines3[extra] pybullet\n",
        "!pip3 install numpngw\n",
        "import gym \n",
        "import numpy as np \n",
        "import torch.nn as nn\n",
        "import copy \n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from numpngw import write_apng\n",
        "from IPython.display import Image\n",
        "from numpngw import write_apng\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.6/dist-packages (0.10.0)\n",
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.6/dist-packages (3.0.7)\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (0.17.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (1.1.5)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (1.18.5)\n",
            "Requirement already satisfied: tensorboard; extra == \"extra\" in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (2.3.0)\n",
            "Requirement already satisfied: opencv-python; extra == \"extra\" in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (4.1.2.30)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"extra\" in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (0.2.6)\n",
            "Requirement already satisfied: pillow; extra == \"extra\" in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (7.0.0)\n",
            "Requirement already satisfied: psutil; extra == \"extra\" in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.17->stable-baselines3[extra]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.17->stable-baselines3[extra]) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines3[extra]) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines3[extra]) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3[extra]) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3[extra]) (0.10.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->stable-baselines3[extra]) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->stable-baselines3[extra]) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->stable-baselines3[extra]) (0.8)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (50.3.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.36.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.7.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.12.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.17.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.34.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.4.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.1.0)\n",
            "Requirement already satisfied: numpngw in /usr/local/lib/python3.6/dist-packages (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from numpngw) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMAhrTUH9LFk"
      },
      "source": [
        "\"\"\" Modify Pendulum Environment to be able to use a mlp reward \"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import error, spaces, utils\n",
        "from gym.utils import seeding\n",
        "from gym.envs.classic_control.pendulum import PendulumEnv\n",
        "import gym.envs.classic_control.pendulum\n",
        "\n",
        "def angle_normalize(x):\n",
        "    return (((x+np.pi) % (2*np.pi)) - np.pi)\n",
        "\n",
        "class PendulumSSRLEnv(PendulumEnv):\n",
        "    def __init__(self):\n",
        "        super(PendulumSSRLEnv, self).__init__()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.RewardFunction = RewardNet(input_size=4).to(device=self.device)\n",
        "        self._max_episode_steps = 200\n",
        "\n",
        "    def step(self, u):\n",
        "        th, thdot = self.state  # th := theta\n",
        "\n",
        "        g = self.g\n",
        "        m = self.m\n",
        "        l = self.l\n",
        "        dt = self.dt\n",
        "\n",
        "        u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
        "        self.last_u = u  # for rendering\n",
        "        costs = angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2)\n",
        "\n",
        "        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt\n",
        "        newth = th + newthdot * dt\n",
        "        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)\n",
        "\n",
        "        self.state = np.array([newth, newthdot])\n",
        "\n",
        "        #### Here we use our own reward function and call that to approximate -cost\n",
        "        reward = self.calc_reward(self._get_obs(), u)\n",
        "        reward = float(reward)\n",
        "        return self._get_obs(), reward, False, {}\n",
        "\n",
        "    def calc_reward(self, obs, action):\n",
        "        \"\"\" Approximate the reward for obs,action combo with neural net\n",
        "        Args:\n",
        "            obs: trig funcs of angle and angular velocity, an array [cos(th),sin(th),thdot]\n",
        "            action: torque, float\n",
        "        Returns:\n",
        "            reward: approx reward, float \n",
        "        \"\"\"\n",
        "        input = np.array([obs[0], obs[1], obs[2], action])\n",
        "        input = torch.from_numpy(input).float().to(device=self.device)\n",
        "        return self.RewardFunction(input)\n",
        "\n",
        "    def set_reward_func(self, model):\n",
        "        \"\"\" set the reward mlp (from outside class)\n",
        "        Args: \n",
        "            model: reward mlp, pytorch neural network  \n",
        "        \"\"\"\n",
        "        self.RewardFunction = model \n",
        "\n",
        "    def get_reward_func(self):\n",
        "        \"\"\" Return the reward mlp outside of class\n",
        "        Return: \n",
        "            model: reward mlp, pytorch neural network  \n",
        "        \"\"\"\n",
        "        return self.RewardFunction"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AomN8-ii9hFC"
      },
      "source": [
        "\"\"\" Functions for creating and training Reward Neural Network\"\"\"\n",
        "\n",
        "# Calculate sum of rewards for one episode \n",
        "def get_return(model, trajectory):\n",
        "    num_steps = trajectory.shape[-1]\n",
        "    return_sum = 0\n",
        "    for i in range(0, num_steps, 4):\n",
        "        # return_sum += model(torch.from_numpy(trajectory[i:i+4]).float().to(device)))\n",
        "        return_sum += model(trajectory[i:i+4])\n",
        "\n",
        "    return return_sum \n",
        "\n",
        "class RewardLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RewardLoss, self).__init__()\n",
        "        self.max_torque = 2\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def forward(self, D_piRL, D_samp, reward_model):\n",
        "        \"\"\" Calculate (negative) log-likelihood of trajectories, reward model parameters   \n",
        "        D_piRL: tensor of size (N, T*(size of action space + size of observation space))\n",
        "        D_samp: tensor of size (M, T*(size of action space + size of observation space))\n",
        "                Where, \n",
        "                    N = num samples from piRL \n",
        "                    M = num samples from piTheta (and piRL)\n",
        "                    T = episode length, \n",
        "        reward_model: mlp, current estimate of reward function \n",
        "        return: Negative of eq. 3 from Generalizing Skills paper \n",
        "        \"\"\"\n",
        "        D_piRL = D_piRL.to(device=self.device)\n",
        "        D_samp = D_samp.to(device=self.device)\n",
        "        reward_model = reward_model.to(device=self.device)\n",
        "        \n",
        "        # Estimate partition function, Z \n",
        "        second_term = 0 \n",
        "        for i in range(D_samp.shape[0]):\n",
        "\n",
        "            numerator = torch.exp(get_return(reward_model, D_samp[i,:]))\n",
        "            denominator = 1 / (self.max_torque - (-1.0 * self.max_torque))\n",
        "            second_term += (numerator/denominator)\n",
        "        \n",
        "        log_second_term = torch.log(second_term)\n",
        "\n",
        "        # Now estimate Likelihood\n",
        "        first_term = 0 \n",
        "        for i in range(D_piRL.shape[0]):\n",
        "            first_term += get_return(reward_model, D_piRL[i,:]) \n",
        "\n",
        "        return -1.0 * (first_term - log_second_term)\n",
        "\n",
        "class RewardNet(nn.Module):\n",
        "    def __init__(self, input_size, h1_size=30, h2_size=30):\n",
        "        super(RewardNet, self).__init__()\n",
        "\n",
        "        # Inputs \n",
        "        self.input_size = input_size\n",
        "        self.h1_size = h1_size\n",
        "        self.h2_size = h2_size\n",
        "\n",
        "        # Fully Connected Layers \n",
        "        self.linear1 = nn.Linear(self.input_size, self.h1_size)\n",
        "        self.linear2 = nn.Linear(self.h1_size, self.h2_size)\n",
        "        self.linear3 = nn.Linear(self.h2_size, 1)\n",
        "\n",
        "        # Activations\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, input_data):\n",
        "        out = self.relu(self.linear1(input_data))\n",
        "        out = self.relu(self.linear2(out))\n",
        "        return self.linear3(out)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRaiKijB74wX"
      },
      "source": [
        "def train_policy(env, policy, num_policy_train_steps, start_timesteps, eval_freq, evaluations, episode_rewards, losses, env_id=\"Custom Environment\", seed=0):\n",
        "  \"\"\" Function that performs TD3 training algorithm \"\"\"\n",
        "  #Define all the hyperparameters (currently default from the repo this code was taken)\n",
        "  expl_noise = 0.1  # Std of Gaussian exploration noise\n",
        "  policy_noise = 0.2 # Noise added to target policy during critic update\n",
        "  batch_size = 256 # Batch size for both actor and critic\n",
        "  noise_clip = 0.5 # Range to clip target policy noise\n",
        "  policy_freq = 2 # Frequency of delayed policy updates\n",
        "  tau = 0.005 # Target network update rate\n",
        "  discount = 0.99 # Discount factor\n",
        "\n",
        "  state_dim = env.observation_space.shape[-1]\n",
        "  action_dim = env.action_space.shape[-1]\n",
        "\n",
        "  #Training\n",
        "  replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
        "  \t\n",
        "  # Evaluate untrained policy  \n",
        "  state, done = env.reset(), False\n",
        "  episode_reward = 0\n",
        "  episode_timesteps = 0\n",
        "  episode_num = 0\n",
        "  \n",
        "  for t in range(int(num_policy_train_steps)):\n",
        "    \n",
        "    episode_timesteps += 1\n",
        "  \n",
        "    # Select action randomly or according to policy\n",
        "    if t < start_timesteps:\n",
        "      action = env.action_space.sample()\n",
        "    else:\n",
        "      action = (\n",
        "        policy.select_action(np.array(state))\n",
        "        + np.random.normal(0, max_action * expl_noise, size=action_dim)\n",
        "      ).clip(-max_action, max_action)\n",
        "  \n",
        "    # Perform action\n",
        "    next_state, reward, done, _ = env.step(action) \n",
        "    done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
        "  \n",
        "    # Store data in replay buffer\n",
        "    replay_buffer.add(state, action, next_state, reward, done_bool)\n",
        "  \n",
        "    state = next_state\n",
        "    episode_reward += reward\n",
        "  \n",
        "    # Train agent after collecting sufficient data\n",
        "    if t >= start_timesteps:\n",
        "      loss_critic, loss_actor = policy.train(replay_buffer, batch_size)\n",
        "      losses.append([loss_critic, loss_actor, t+1]) #saving for plotting\n",
        "  \n",
        "    if done: \n",
        "      # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
        "      print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
        "      episode_rewards.append([episode_reward, t+1]) #saving for plotting\n",
        "      # Reset environment\n",
        "      state, done = env.reset(), False\n",
        "      episode_reward = 0\n",
        "      episode_timesteps = 0\n",
        "      episode_num += 1 \n",
        "  \n",
        "    # Evaluate episode\n",
        "    if (t + 1) % eval_freq == 0:\n",
        "      evaluations.append([eval_policy(policy, env_id, seed), t+1] )\n",
        "      #np.save(f\"results_{t+1}_{file_name}\", evaluations)\n",
        "      #if save_model:\n",
        "      #   policy.save(f\"models_{t+1}_{file_name}\")\n",
        "      #  #  files.download(f'{name}.zip') \n",
        "  return policy, losses, episode_rewards, evaluations \n",
        "\n",
        "def train_reward(model, D_piRL, D_samp, losses, num_reward_train_steps=10, batch_size=4):\n",
        "\n",
        "    \"\"\" Train the reward model for a specified number of training steps \n",
        "        Args:\n",
        "            model: reward mlp, pytorch neural network  \n",
        "            D_piRL: supervised environment samples \n",
        "            D_samp: samples also from new policy \n",
        "            num_reward_train_steps: how long to train, int \n",
        "            batch_size: size of batch, int\n",
        "        Return:\n",
        "            model: updated model\n",
        "        \"\"\"\n",
        "    model = model.to(device)\n",
        "    loss_func = RewardLoss()\n",
        "    loss_func.requires_grad = True\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    for step in range(num_reward_train_steps):\n",
        "\n",
        "        # Randomly select batch of trajectories from D_piRL and D_samp\n",
        "        i = np.random.choice(D_piRL.shape[0], size=batch_size, replace=False)\n",
        "        j = np.random.choice(D_samp.shape[0], size=batch_size, replace=False) \n",
        "        trajectories_piRL = torch.from_numpy(D_piRL[i].astype(np.float32)).float()\n",
        "        trajectories_samp = torch.from_numpy(D_samp[j].astype(np.float32)).float()\n",
        "        \n",
        "        # Compute Loss and train \n",
        "        loss = loss_func(trajectories_piRL, trajectories_samp, model)\n",
        "        print(\"Loss: \", loss)\n",
        "        optimizer.zero_grad()  \n",
        "        loss.backward()        \n",
        "        optimizer.step()\n",
        "        losses.append([loss.detach().cpu().numpy(), step]) #saving for plotting      \n",
        "\n",
        "    return model, losses"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9aDk40TuqSV"
      },
      "source": [
        "def generate_samples(env, model, num_samples):\n",
        "    \"\"\" Generate specified number of samples using a specific model in specific \n",
        "        environment\n",
        "    Args: \n",
        "        env: gym environment\n",
        "        model: mlp policy, from TD3 code \n",
        "        num_samples: size of set, int \n",
        "    \"\"\"\n",
        "    # Define how long the episodes are and the size of the state-action combo \n",
        "    _max_episode_steps = env._max_episode_steps\n",
        "    print(_max_episode_steps)\n",
        "    obs_action_size = env.action_space.shape[-1] + env.observation_space.shape[-1]\n",
        "    D = np.empty([num_samples, _max_episode_steps * obs_action_size])\n",
        "    for i in range(num_samples):\n",
        "        sample = np.empty(0)\n",
        "        obs = env.reset()\n",
        "        # for t in range(0, _max_episode_steps, obs_action_size):\n",
        "        for t in range(0, _max_episode_steps):\n",
        "            action = model.select_action(np.array(obs))\n",
        "            sample = np.append(sample, np.append(obs, action))\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "        D[i,:] = sample\n",
        "    return D "
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTF3nBC2MJdo"
      },
      "source": [
        "# Train piTheta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQZOSsECrVzD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1febe7ce-7832-4b6d-e489-a8f4d3cfa6b3"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "## Train piRL using standard pendulum environment\n",
        "\n",
        "# Create the environment\n",
        "standard_env_id = \"Pendulum-v0\"\n",
        "env_standard = gym.make(standard_env_id)\n",
        "state_dim = env_standard.observation_space.shape[-1]\n",
        "action_dim = env_standard.action_space.shape[-1]\n",
        "max_action=env_standard.max_torque\n",
        "print('State dimensions', state_dim)\n",
        "print('Action dimensions', action_dim)\n",
        "print('Max action', max_action)\n",
        "print('Max number of episodes', env_standard._max_episode_steps)\n",
        "\n",
        "# Seed environment, torch, and numpy for consistent results \n",
        "seed=0 # Sets Gym, PyTorch and Numpy seeds\n",
        "env_standard.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Define all the hyperparameters (currently default from the repo this code was taken)\n",
        "start_timesteps = 1e3 # Time steps initial random policy is used\n",
        "policy_noise = 0.2 # Noise added to target policy during critic update\n",
        "noise_clip = 0.5 # Range to clip target policy noise\n",
        "policy_freq = 2 # Frequency of delayed policy updates\n",
        "tau = 0.005 # Target network update rate\n",
        "discount = 0.99 # Discount factor\n",
        "eval_freq = 1e3 # How often (time steps) we evaluate\n",
        "\n",
        "kwargs = {\n",
        "\t\t\"state_dim\": state_dim,\n",
        "\t\t\"action_dim\": action_dim,\n",
        "\t\t\"max_action\": max_action,\n",
        "\t\t\"discount\": discount,\n",
        "\t\t\"tau\": tau,\n",
        "\t}\n",
        "\n",
        "# Target policy smoothing is scaled wrt the action scale\n",
        "kwargs[\"policy_noise\"] = policy_noise * max_action\n",
        "kwargs[\"noise_clip\"] = noise_clip * max_action\n",
        "kwargs[\"policy_freq\"] = policy_freq\n",
        "policy_RL = TD3(**kwargs)\n",
        "evaluations = []\n",
        "episode_rewards = []\n",
        "losses = []\n",
        "\n",
        "policy_RL, losses, episode_rewards, evaluations = train_policy(env_standard, policy_RL, \n",
        "                         num_policy_train_steps=5e4, \n",
        "                         start_timesteps=10e3,\n",
        "                         eval_freq=5e3,\n",
        "                         evaluations=evaluations,\n",
        "                         episode_rewards=episode_rewards,\n",
        "                         losses=losses,\n",
        "                         env_id=standard_env_id,\n",
        "                         seed=0)\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State dimensions 3\n",
            "Action dimensions 1\n",
            "Max action 2.0\n",
            "Max number of episodes 200\n",
            "Total T: 200 Episode Num: 1 Episode T: 200 Reward: -1679.530\n",
            "Total T: 400 Episode Num: 2 Episode T: 200 Reward: -1145.897\n",
            "Total T: 600 Episode Num: 3 Episode T: 200 Reward: -1723.053\n",
            "Total T: 800 Episode Num: 4 Episode T: 200 Reward: -998.338\n",
            "Total T: 1000 Episode Num: 5 Episode T: 200 Reward: -1665.440\n",
            "Total T: 1200 Episode Num: 6 Episode T: 200 Reward: -1602.659\n",
            "Total T: 1400 Episode Num: 7 Episode T: 200 Reward: -1156.734\n",
            "Total T: 1600 Episode Num: 8 Episode T: 200 Reward: -1758.571\n",
            "Total T: 1800 Episode Num: 9 Episode T: 200 Reward: -953.697\n",
            "Total T: 2000 Episode Num: 10 Episode T: 200 Reward: -1582.796\n",
            "Total T: 2200 Episode Num: 11 Episode T: 200 Reward: -1285.653\n",
            "Total T: 2400 Episode Num: 12 Episode T: 200 Reward: -1044.318\n",
            "Total T: 2600 Episode Num: 13 Episode T: 200 Reward: -1661.699\n",
            "Total T: 2800 Episode Num: 14 Episode T: 200 Reward: -1169.263\n",
            "Total T: 3000 Episode Num: 15 Episode T: 200 Reward: -789.660\n",
            "Total T: 3200 Episode Num: 16 Episode T: 200 Reward: -860.333\n",
            "Total T: 3400 Episode Num: 17 Episode T: 200 Reward: -858.191\n",
            "Total T: 3600 Episode Num: 18 Episode T: 200 Reward: -856.124\n",
            "Total T: 3800 Episode Num: 19 Episode T: 200 Reward: -1426.830\n",
            "Total T: 4000 Episode Num: 20 Episode T: 200 Reward: -974.585\n",
            "Total T: 4200 Episode Num: 21 Episode T: 200 Reward: -1168.390\n",
            "Total T: 4400 Episode Num: 22 Episode T: 200 Reward: -1789.149\n",
            "Total T: 4600 Episode Num: 23 Episode T: 200 Reward: -1704.370\n",
            "Total T: 4800 Episode Num: 24 Episode T: 200 Reward: -1504.998\n",
            "Total T: 5000 Episode Num: 25 Episode T: 200 Reward: -1049.728\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -1690.832\n",
            "---------------------------------------\n",
            "Total T: 5200 Episode Num: 26 Episode T: 200 Reward: -1598.384\n",
            "Total T: 5400 Episode Num: 27 Episode T: 200 Reward: -1324.588\n",
            "Total T: 5600 Episode Num: 28 Episode T: 200 Reward: -1193.462\n",
            "Total T: 5800 Episode Num: 29 Episode T: 200 Reward: -868.784\n",
            "Total T: 6000 Episode Num: 30 Episode T: 200 Reward: -1632.961\n",
            "Total T: 6200 Episode Num: 31 Episode T: 200 Reward: -1511.542\n",
            "Total T: 6400 Episode Num: 32 Episode T: 200 Reward: -1713.575\n",
            "Total T: 6600 Episode Num: 33 Episode T: 200 Reward: -824.855\n",
            "Total T: 6800 Episode Num: 34 Episode T: 200 Reward: -1171.539\n",
            "Total T: 7000 Episode Num: 35 Episode T: 200 Reward: -1504.337\n",
            "Total T: 7200 Episode Num: 36 Episode T: 200 Reward: -1769.978\n",
            "Total T: 7400 Episode Num: 37 Episode T: 200 Reward: -1678.648\n",
            "Total T: 7600 Episode Num: 38 Episode T: 200 Reward: -808.623\n",
            "Total T: 7800 Episode Num: 39 Episode T: 200 Reward: -1178.315\n",
            "Total T: 8000 Episode Num: 40 Episode T: 200 Reward: -1319.046\n",
            "Total T: 8200 Episode Num: 41 Episode T: 200 Reward: -1639.001\n",
            "Total T: 8400 Episode Num: 42 Episode T: 200 Reward: -1694.217\n",
            "Total T: 8600 Episode Num: 43 Episode T: 200 Reward: -1177.964\n",
            "Total T: 8800 Episode Num: 44 Episode T: 200 Reward: -895.676\n",
            "Total T: 9000 Episode Num: 45 Episode T: 200 Reward: -877.472\n",
            "Total T: 9200 Episode Num: 46 Episode T: 200 Reward: -1084.503\n",
            "Total T: 9400 Episode Num: 47 Episode T: 200 Reward: -1397.931\n",
            "Total T: 9600 Episode Num: 48 Episode T: 200 Reward: -1076.371\n",
            "Total T: 9800 Episode Num: 49 Episode T: 200 Reward: -920.488\n",
            "Total T: 10000 Episode Num: 50 Episode T: 200 Reward: -981.016\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -1690.832\n",
            "---------------------------------------\n",
            "Total T: 10200 Episode Num: 51 Episode T: 200 Reward: -1761.479\n",
            "Total T: 10400 Episode Num: 52 Episode T: 200 Reward: -1313.024\n",
            "Total T: 10600 Episode Num: 53 Episode T: 200 Reward: -1833.349\n",
            "Total T: 10800 Episode Num: 54 Episode T: 200 Reward: -1784.588\n",
            "Total T: 11000 Episode Num: 55 Episode T: 200 Reward: -1581.878\n",
            "Total T: 11200 Episode Num: 56 Episode T: 200 Reward: -1582.701\n",
            "Total T: 11400 Episode Num: 57 Episode T: 200 Reward: -1425.242\n",
            "Total T: 11600 Episode Num: 58 Episode T: 200 Reward: -1498.295\n",
            "Total T: 11800 Episode Num: 59 Episode T: 200 Reward: -1484.457\n",
            "Total T: 12000 Episode Num: 60 Episode T: 200 Reward: -1479.673\n",
            "Total T: 12200 Episode Num: 61 Episode T: 200 Reward: -1413.178\n",
            "Total T: 12400 Episode Num: 62 Episode T: 200 Reward: -1365.021\n",
            "Total T: 12600 Episode Num: 63 Episode T: 200 Reward: -1278.081\n",
            "Total T: 12800 Episode Num: 64 Episode T: 200 Reward: -1200.306\n",
            "Total T: 13000 Episode Num: 65 Episode T: 200 Reward: -1128.546\n",
            "Total T: 13200 Episode Num: 66 Episode T: 200 Reward: -1027.734\n",
            "Total T: 13400 Episode Num: 67 Episode T: 200 Reward: -898.604\n",
            "Total T: 13600 Episode Num: 68 Episode T: 200 Reward: -1003.208\n",
            "Total T: 13800 Episode Num: 69 Episode T: 200 Reward: -1089.568\n",
            "Total T: 14000 Episode Num: 70 Episode T: 200 Reward: -563.809\n",
            "Total T: 14200 Episode Num: 71 Episode T: 200 Reward: -397.708\n",
            "Total T: 14400 Episode Num: 72 Episode T: 200 Reward: -268.033\n",
            "Total T: 14600 Episode Num: 73 Episode T: 200 Reward: -396.832\n",
            "Total T: 14800 Episode Num: 74 Episode T: 200 Reward: -3.466\n",
            "Total T: 15000 Episode Num: 75 Episode T: 200 Reward: -260.501\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -410.055\n",
            "---------------------------------------\n",
            "Total T: 15200 Episode Num: 76 Episode T: 200 Reward: -3.189\n",
            "Total T: 15400 Episode Num: 77 Episode T: 200 Reward: -252.393\n",
            "Total T: 15600 Episode Num: 78 Episode T: 200 Reward: -128.528\n",
            "Total T: 15800 Episode Num: 79 Episode T: 200 Reward: -127.041\n",
            "Total T: 16000 Episode Num: 80 Episode T: 200 Reward: -128.923\n",
            "Total T: 16200 Episode Num: 81 Episode T: 200 Reward: -310.385\n",
            "Total T: 16400 Episode Num: 82 Episode T: 200 Reward: -2.464\n",
            "Total T: 16600 Episode Num: 83 Episode T: 200 Reward: -368.267\n",
            "Total T: 16800 Episode Num: 84 Episode T: 200 Reward: -131.108\n",
            "Total T: 17000 Episode Num: 85 Episode T: 200 Reward: -127.800\n",
            "Total T: 17200 Episode Num: 86 Episode T: 200 Reward: -124.300\n",
            "Total T: 17400 Episode Num: 87 Episode T: 200 Reward: -362.706\n",
            "Total T: 17600 Episode Num: 88 Episode T: 200 Reward: -120.023\n",
            "Total T: 17800 Episode Num: 89 Episode T: 200 Reward: -127.361\n",
            "Total T: 18000 Episode Num: 90 Episode T: 200 Reward: -120.616\n",
            "Total T: 18200 Episode Num: 91 Episode T: 200 Reward: -124.946\n",
            "Total T: 18400 Episode Num: 92 Episode T: 200 Reward: -120.842\n",
            "Total T: 18600 Episode Num: 93 Episode T: 200 Reward: -116.018\n",
            "Total T: 18800 Episode Num: 94 Episode T: 200 Reward: -124.881\n",
            "Total T: 19000 Episode Num: 95 Episode T: 200 Reward: -0.917\n",
            "Total T: 19200 Episode Num: 96 Episode T: 200 Reward: -241.699\n",
            "Total T: 19400 Episode Num: 97 Episode T: 200 Reward: -125.991\n",
            "Total T: 19600 Episode Num: 98 Episode T: 200 Reward: -118.664\n",
            "Total T: 19800 Episode Num: 99 Episode T: 200 Reward: -119.888\n",
            "Total T: 20000 Episode Num: 100 Episode T: 200 Reward: -343.488\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -229.782\n",
            "---------------------------------------\n",
            "Total T: 20200 Episode Num: 101 Episode T: 200 Reward: -118.764\n",
            "Total T: 20400 Episode Num: 102 Episode T: 200 Reward: -116.292\n",
            "Total T: 20600 Episode Num: 103 Episode T: 200 Reward: -239.982\n",
            "Total T: 20800 Episode Num: 104 Episode T: 200 Reward: -354.659\n",
            "Total T: 21000 Episode Num: 105 Episode T: 200 Reward: -359.701\n",
            "Total T: 21200 Episode Num: 106 Episode T: 200 Reward: -120.466\n",
            "Total T: 21400 Episode Num: 107 Episode T: 200 Reward: -119.381\n",
            "Total T: 21600 Episode Num: 108 Episode T: 200 Reward: -233.559\n",
            "Total T: 21800 Episode Num: 109 Episode T: 200 Reward: -119.004\n",
            "Total T: 22000 Episode Num: 110 Episode T: 200 Reward: -234.822\n",
            "Total T: 22200 Episode Num: 111 Episode T: 200 Reward: -128.168\n",
            "Total T: 22400 Episode Num: 112 Episode T: 200 Reward: -126.157\n",
            "Total T: 22600 Episode Num: 113 Episode T: 200 Reward: -233.557\n",
            "Total T: 22800 Episode Num: 114 Episode T: 200 Reward: -118.828\n",
            "Total T: 23000 Episode Num: 115 Episode T: 200 Reward: -120.707\n",
            "Total T: 23200 Episode Num: 116 Episode T: 200 Reward: -229.514\n",
            "Total T: 23400 Episode Num: 117 Episode T: 200 Reward: -251.915\n",
            "Total T: 23600 Episode Num: 118 Episode T: 200 Reward: -124.875\n",
            "Total T: 23800 Episode Num: 119 Episode T: 200 Reward: -118.900\n",
            "Total T: 24000 Episode Num: 120 Episode T: 200 Reward: -127.112\n",
            "Total T: 24200 Episode Num: 121 Episode T: 200 Reward: -373.587\n",
            "Total T: 24400 Episode Num: 122 Episode T: 200 Reward: -118.825\n",
            "Total T: 24600 Episode Num: 123 Episode T: 200 Reward: -118.289\n",
            "Total T: 24800 Episode Num: 124 Episode T: 200 Reward: -265.072\n",
            "Total T: 25000 Episode Num: 125 Episode T: 200 Reward: -121.867\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -235.554\n",
            "---------------------------------------\n",
            "Total T: 25200 Episode Num: 126 Episode T: 200 Reward: -241.614\n",
            "Total T: 25400 Episode Num: 127 Episode T: 200 Reward: -122.350\n",
            "Total T: 25600 Episode Num: 128 Episode T: 200 Reward: -122.345\n",
            "Total T: 25800 Episode Num: 129 Episode T: 200 Reward: -119.633\n",
            "Total T: 26000 Episode Num: 130 Episode T: 200 Reward: -227.878\n",
            "Total T: 26200 Episode Num: 131 Episode T: 200 Reward: -1.458\n",
            "Total T: 26400 Episode Num: 132 Episode T: 200 Reward: -234.045\n",
            "Total T: 26600 Episode Num: 133 Episode T: 200 Reward: -122.372\n",
            "Total T: 26800 Episode Num: 134 Episode T: 200 Reward: -116.849\n",
            "Total T: 27000 Episode Num: 135 Episode T: 200 Reward: -252.633\n",
            "Total T: 27200 Episode Num: 136 Episode T: 200 Reward: -306.002\n",
            "Total T: 27400 Episode Num: 137 Episode T: 200 Reward: -116.240\n",
            "Total T: 27600 Episode Num: 138 Episode T: 200 Reward: -228.957\n",
            "Total T: 27800 Episode Num: 139 Episode T: 200 Reward: -119.378\n",
            "Total T: 28000 Episode Num: 140 Episode T: 200 Reward: -231.520\n",
            "Total T: 28200 Episode Num: 141 Episode T: 200 Reward: -118.371\n",
            "Total T: 28400 Episode Num: 142 Episode T: 200 Reward: -230.304\n",
            "Total T: 28600 Episode Num: 143 Episode T: 200 Reward: -302.941\n",
            "Total T: 28800 Episode Num: 144 Episode T: 200 Reward: -119.148\n",
            "Total T: 29000 Episode Num: 145 Episode T: 200 Reward: -118.686\n",
            "Total T: 29200 Episode Num: 146 Episode T: 200 Reward: -119.489\n",
            "Total T: 29400 Episode Num: 147 Episode T: 200 Reward: -1.886\n",
            "Total T: 29600 Episode Num: 148 Episode T: 200 Reward: -120.064\n",
            "Total T: 29800 Episode Num: 149 Episode T: 200 Reward: -2.728\n",
            "Total T: 30000 Episode Num: 150 Episode T: 200 Reward: -226.852\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -230.656\n",
            "---------------------------------------\n",
            "Total T: 30200 Episode Num: 151 Episode T: 200 Reward: -358.688\n",
            "Total T: 30400 Episode Num: 152 Episode T: 200 Reward: -232.666\n",
            "Total T: 30600 Episode Num: 153 Episode T: 200 Reward: -1.316\n",
            "Total T: 30800 Episode Num: 154 Episode T: 200 Reward: -114.554\n",
            "Total T: 31000 Episode Num: 155 Episode T: 200 Reward: -120.237\n",
            "Total T: 31200 Episode Num: 156 Episode T: 200 Reward: -237.301\n",
            "Total T: 31400 Episode Num: 157 Episode T: 200 Reward: -114.894\n",
            "Total T: 31600 Episode Num: 158 Episode T: 200 Reward: -122.673\n",
            "Total T: 31800 Episode Num: 159 Episode T: 200 Reward: -116.312\n",
            "Total T: 32000 Episode Num: 160 Episode T: 200 Reward: -121.392\n",
            "Total T: 32200 Episode Num: 161 Episode T: 200 Reward: -226.127\n",
            "Total T: 32400 Episode Num: 162 Episode T: 200 Reward: -119.762\n",
            "Total T: 32600 Episode Num: 163 Episode T: 200 Reward: -337.076\n",
            "Total T: 32800 Episode Num: 164 Episode T: 200 Reward: -117.971\n",
            "Total T: 33000 Episode Num: 165 Episode T: 200 Reward: -119.951\n",
            "Total T: 33200 Episode Num: 166 Episode T: 200 Reward: -120.038\n",
            "Total T: 33400 Episode Num: 167 Episode T: 200 Reward: -118.585\n",
            "Total T: 33600 Episode Num: 168 Episode T: 200 Reward: -340.762\n",
            "Total T: 33800 Episode Num: 169 Episode T: 200 Reward: -119.553\n",
            "Total T: 34000 Episode Num: 170 Episode T: 200 Reward: -118.708\n",
            "Total T: 34200 Episode Num: 171 Episode T: 200 Reward: -333.283\n",
            "Total T: 34400 Episode Num: 172 Episode T: 200 Reward: -120.106\n",
            "Total T: 34600 Episode Num: 173 Episode T: 200 Reward: -123.597\n",
            "Total T: 34800 Episode Num: 174 Episode T: 200 Reward: -310.926\n",
            "Total T: 35000 Episode Num: 175 Episode T: 200 Reward: -117.864\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -229.433\n",
            "---------------------------------------\n",
            "Total T: 35200 Episode Num: 176 Episode T: 200 Reward: -226.899\n",
            "Total T: 35400 Episode Num: 177 Episode T: 200 Reward: -3.655\n",
            "Total T: 35600 Episode Num: 178 Episode T: 200 Reward: -242.156\n",
            "Total T: 35800 Episode Num: 179 Episode T: 200 Reward: -121.079\n",
            "Total T: 36000 Episode Num: 180 Episode T: 200 Reward: -229.108\n",
            "Total T: 36200 Episode Num: 181 Episode T: 200 Reward: -342.999\n",
            "Total T: 36400 Episode Num: 182 Episode T: 200 Reward: -118.175\n",
            "Total T: 36600 Episode Num: 183 Episode T: 200 Reward: -116.820\n",
            "Total T: 36800 Episode Num: 184 Episode T: 200 Reward: -118.922\n",
            "Total T: 37000 Episode Num: 185 Episode T: 200 Reward: -230.299\n",
            "Total T: 37200 Episode Num: 186 Episode T: 200 Reward: -123.050\n",
            "Total T: 37400 Episode Num: 187 Episode T: 200 Reward: -119.001\n",
            "Total T: 37600 Episode Num: 188 Episode T: 200 Reward: -1.528\n",
            "Total T: 37800 Episode Num: 189 Episode T: 200 Reward: -228.644\n",
            "Total T: 38000 Episode Num: 190 Episode T: 200 Reward: -0.520\n",
            "Total T: 38200 Episode Num: 191 Episode T: 200 Reward: -114.917\n",
            "Total T: 38400 Episode Num: 192 Episode T: 200 Reward: -221.685\n",
            "Total T: 38600 Episode Num: 193 Episode T: 200 Reward: -116.947\n",
            "Total T: 38800 Episode Num: 194 Episode T: 200 Reward: -314.514\n",
            "Total T: 39000 Episode Num: 195 Episode T: 200 Reward: -124.036\n",
            "Total T: 39200 Episode Num: 196 Episode T: 200 Reward: -231.977\n",
            "Total T: 39400 Episode Num: 197 Episode T: 200 Reward: -120.611\n",
            "Total T: 39600 Episode Num: 198 Episode T: 200 Reward: -230.449\n",
            "Total T: 39800 Episode Num: 199 Episode T: 200 Reward: -239.296\n",
            "Total T: 40000 Episode Num: 200 Episode T: 200 Reward: -126.904\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -229.760\n",
            "---------------------------------------\n",
            "Total T: 40200 Episode Num: 201 Episode T: 200 Reward: -240.743\n",
            "Total T: 40400 Episode Num: 202 Episode T: 200 Reward: -116.962\n",
            "Total T: 40600 Episode Num: 203 Episode T: 200 Reward: -124.809\n",
            "Total T: 40800 Episode Num: 204 Episode T: 200 Reward: -119.313\n",
            "Total T: 41000 Episode Num: 205 Episode T: 200 Reward: -119.108\n",
            "Total T: 41200 Episode Num: 206 Episode T: 200 Reward: -125.427\n",
            "Total T: 41400 Episode Num: 207 Episode T: 200 Reward: -125.691\n",
            "Total T: 41600 Episode Num: 208 Episode T: 200 Reward: -117.337\n",
            "Total T: 41800 Episode Num: 209 Episode T: 200 Reward: -116.557\n",
            "Total T: 42000 Episode Num: 210 Episode T: 200 Reward: -0.748\n",
            "Total T: 42200 Episode Num: 211 Episode T: 200 Reward: -120.557\n",
            "Total T: 42400 Episode Num: 212 Episode T: 200 Reward: -116.510\n",
            "Total T: 42600 Episode Num: 213 Episode T: 200 Reward: -120.020\n",
            "Total T: 42800 Episode Num: 214 Episode T: 200 Reward: -0.769\n",
            "Total T: 43000 Episode Num: 215 Episode T: 200 Reward: -337.902\n",
            "Total T: 43200 Episode Num: 216 Episode T: 200 Reward: -0.566\n",
            "Total T: 43400 Episode Num: 217 Episode T: 200 Reward: -1.953\n",
            "Total T: 43600 Episode Num: 218 Episode T: 200 Reward: -118.319\n",
            "Total T: 43800 Episode Num: 219 Episode T: 200 Reward: -238.894\n",
            "Total T: 44000 Episode Num: 220 Episode T: 200 Reward: -244.766\n",
            "Total T: 44200 Episode Num: 221 Episode T: 200 Reward: -119.053\n",
            "Total T: 44400 Episode Num: 222 Episode T: 200 Reward: -120.371\n",
            "Total T: 44600 Episode Num: 223 Episode T: 200 Reward: -0.541\n",
            "Total T: 44800 Episode Num: 224 Episode T: 200 Reward: -233.254\n",
            "Total T: 45000 Episode Num: 225 Episode T: 200 Reward: -229.208\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -223.505\n",
            "---------------------------------------\n",
            "Total T: 45200 Episode Num: 226 Episode T: 200 Reward: -129.494\n",
            "Total T: 45400 Episode Num: 227 Episode T: 200 Reward: -372.699\n",
            "Total T: 45600 Episode Num: 228 Episode T: 200 Reward: -119.078\n",
            "Total T: 45800 Episode Num: 229 Episode T: 200 Reward: -244.828\n",
            "Total T: 46000 Episode Num: 230 Episode T: 200 Reward: -3.308\n",
            "Total T: 46200 Episode Num: 231 Episode T: 200 Reward: -114.043\n",
            "Total T: 46400 Episode Num: 232 Episode T: 200 Reward: -233.579\n",
            "Total T: 46600 Episode Num: 233 Episode T: 200 Reward: -122.513\n",
            "Total T: 46800 Episode Num: 234 Episode T: 200 Reward: -130.726\n",
            "Total T: 47000 Episode Num: 235 Episode T: 200 Reward: -236.436\n",
            "Total T: 47200 Episode Num: 236 Episode T: 200 Reward: -325.248\n",
            "Total T: 47400 Episode Num: 237 Episode T: 200 Reward: -119.184\n",
            "Total T: 47600 Episode Num: 238 Episode T: 200 Reward: -123.639\n",
            "Total T: 47800 Episode Num: 239 Episode T: 200 Reward: -118.352\n",
            "Total T: 48000 Episode Num: 240 Episode T: 200 Reward: -114.124\n",
            "Total T: 48200 Episode Num: 241 Episode T: 200 Reward: -120.884\n",
            "Total T: 48400 Episode Num: 242 Episode T: 200 Reward: -305.419\n",
            "Total T: 48600 Episode Num: 243 Episode T: 200 Reward: -125.217\n",
            "Total T: 48800 Episode Num: 244 Episode T: 200 Reward: -120.102\n",
            "Total T: 49000 Episode Num: 245 Episode T: 200 Reward: -345.923\n",
            "Total T: 49200 Episode Num: 246 Episode T: 200 Reward: -2.331\n",
            "Total T: 49400 Episode Num: 247 Episode T: 200 Reward: -242.881\n",
            "Total T: 49600 Episode Num: 248 Episode T: 200 Reward: -226.993\n",
            "Total T: 49800 Episode Num: 249 Episode T: 200 Reward: -252.217\n",
            "Total T: 50000 Episode Num: 250 Episode T: 200 Reward: -123.438\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -222.965\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaK3u2KxbchW",
        "outputId": "fe78905d-2de5-4ed4-a6bf-80d3374044ba"
      },
      "source": [
        "\n",
        "## Generate samples, D_piRL using piRL \n",
        "D_piRL = generate_samples(env_standard, policy_RL, num_samples=10)\n",
        "\n",
        "## Initialize D_samp to be equal to D_piRL \n",
        "D_samp = D_piRL.copy()"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "MckP8BvESWPK",
        "outputId": "a66d7401-a146-444b-d5bb-7dc088680e85"
      },
      "source": [
        "#Plot rewards during training\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(np.array(episode_rewards)[:,1],np.array(episode_rewards)[:,0], label = 'training reward')\n",
        "plt.plot(np.array(evaluations)[:,1],np.array(evaluations)[:,0], label = 'average evaluation reward')\n",
        "plt.title('Inititial Training of the policy')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Reward')\n",
        "plt.legend()\n",
        "\n",
        "#Plot Losses of Actor and Critic during training\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(np.array(losses)[:,2],np.array(losses)[:,0], label = 'critic loss')\n",
        "# plt.plot(np.array(losses)[::2,2],np.array(losses)[::2,1], label = 'actor loss') #Actor updates every other timestep\n",
        "plt.plot(np.array(losses)[1::2,2],np.array(losses)[1::2,1], label = 'actor loss') #Actor updates every other timestep\n",
        "\n",
        "plt.title('Inititial Training of the policy')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-8b916eeaea2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Plot rewards during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'training reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'average evaluation reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Inititial Training of the policy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ylp39mQD3gHo",
        "outputId": "91ae2844-b87d-4c73-a9dc-f3cb3dc38d43"
      },
      "source": [
        "## Initialize custom environment \n",
        "custom_pendulum_env = PendulumSSRLEnv()\n",
        "check_env(custom_pendulum_env, warn=True)\n",
        "\n",
        "# Seed environment, torch, and numpy for consistent results \n",
        "seed=0 # Sets Gym, PyTorch and Numpy seeds\n",
        "env_standard.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "## Initialize new policy\n",
        "policy_Theta = TD3(**kwargs)\n",
        "\n",
        "## Initialize Reward Function \n",
        "RewardMLP = RewardNet(input_size=4)\n",
        "def weights_init(m):  # Intialize Weights with He Initialization\n",
        "    if (type(m) == torch.nn.Linear):\n",
        "        torch.nn.init.kaiming_uniform_(m.weight)\n",
        "weights_init(RewardMLP)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/stable_baselines3/common/env_checker.py:232: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
            "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeL0qZjq5s5w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b82d7e9d-0d09-4438-f06f-13b95d5109f8"
      },
      "source": [
        "## Run main loop to get the new policy (while learning reward function)\n",
        "I = 30    # total num iterations\n",
        "reward_losses = []\n",
        "evaluations = []\n",
        "episode_rewards = []\n",
        "losses = []\n",
        "for iteration in range(I):\n",
        "    print(\"Iteration: \", (iteration+1))\n",
        "\n",
        "    ## Generate samples, D_piTheta and append to D_samp\n",
        "    D_piTheta = generate_samples(custom_pendulum_env, policy_Theta, num_samples=5)\n",
        "    D_samp = np.concatenate((D_samp,D_piTheta))\n",
        "\n",
        "    ## Train the reward function using D_piRL and D_samp \n",
        "    RewardMLP, reward_losses = train_reward(RewardMLP, D_piRL, D_samp, reward_losses, num_reward_train_steps=100, batch_size=4)\n",
        "\n",
        "    ## Train the policy using the new reward function \n",
        "    custom_pendulum_env.set_reward_func(model=RewardMLP)\n",
        "\n",
        "    policy_Theta, evaluations, episode_rewards, losses, = train_policy(custom_pendulum_env, policy_Theta, \n",
        "                         num_policy_train_steps=5e3, \n",
        "                         start_timesteps=10e2,\n",
        "                         eval_freq=5e2,\n",
        "                         evaluations=evaluations,\n",
        "                         episode_rewards=episode_rewards,\n",
        "                         losses=losses,\n",
        "                         seed=0)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:  1\n",
            "200\n",
            "Loss:  tensor([-31.8908], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([-94.5070], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([-152.2505], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([-216.9605], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([inf], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "Loss:  tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-143-b464cf664938>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                          \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                          \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                          seed=0)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-56-c3ec28604cd9>\u001b[0m in \u001b[0;36mtrain_policy\u001b[0;34m(env, policy, num_policy_train_steps, start_timesteps, eval_freq, evaluations, episode_rewards, losses, env_id, seed)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Evaluate episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0mevaluations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meval_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m       \u001b[0;31m#np.save(f\"results_{t+1}_{file_name}\", evaluations)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0;31m#if save_model:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-11fec09bdfd4>\u001b[0m in \u001b[0;36meval_policy\u001b[0;34m(policy, env_name, seed, eval_episodes)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# A fixed seed is used for the eval environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0meval_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0meval_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get a different seed from before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_id_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to look up malformed environment ID: {}. (Currently all IDs must be of the form {}.)'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_id_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: Attempted to look up malformed environment ID: b'Custom Environment'. (Currently all IDs must be of the form ^(?:[\\w:-]+\\/)?([\\w:.-]+)-v(\\d+)$.)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i4PcTnoYweY"
      },
      "source": [
        "#Plot Losses of Actor and Critic during training\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(np.array(losses)[:,2],np.array(losses)[:,0], label = 'critic loss')\n",
        "# plt.plot(np.array(losses)[::2,2],np.array(losses)[::2,1], label = 'actor loss') #Actor updates every other timestep\n",
        "plt.plot(np.array(losses)[1::2,2],np.array(losses)[1::2,1], label = 'actor loss') #Actor updates every other timestep\n",
        "\n",
        "plt.title('Inverse Reinforcement Learning')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel(' Reward Loss')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXC3oL0lB0JU"
      },
      "source": [
        "eval_env = gym.make(standard_env_id)\n",
        "eval_env.seed(seed + 100) #get a different seed from before\n",
        "\n",
        "avg_reward = 0.\n",
        "actions = np.empty(0)\n",
        "state, done = eval_env.reset(), False\n",
        "while not done:\n",
        "  action = policy_Theta.select_action(np.array(state))\n",
        "  state, reward, done, _ = eval_env.step(action)\n",
        "  avg_reward += reward\n",
        "  actions = np.append(actions, action)\n",
        "  # screen = eval_env.render(mode='rgb_array')\n",
        "\n",
        "np.save('evaluation_actions',actions)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}